{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "colab": {
      "name": "ai2-c1-hw1-students.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ojask12/AutoShot/blob/master/ai2-c1-hw1-students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptgBmE2FVeg9"
      },
      "source": [
        "![](https://github.com/ojask12/ai2-hw1/blob/main/fig/univ.png?raw=1)\n",
        "\n",
        "# AI-2: Convolutional Neural Network\n",
        "## Homework 1: Artificial Neural Networks, Model Interpretation, and Regularization\n",
        "\n",
        "**AI2 Cohort 2**<br/>\n",
        "**Univ.AI**<br/>\n",
        "**Instructor**: Ignacio Becker<br />\n",
        "**Maximum Score**: 100\n",
        "\n",
        "<hr style=\"height:2.4pt\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "kjqMA7UtVehG"
      },
      "outputs": [],
      "source": [
        "#RUN THIS CELL \n",
        "import requests\n",
        "from IPython.core.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, zipfile, io\n",
        "r = requests.get(\"https://github.com/ojask12/ai2-hw1/raw/main/data.zip\")\n",
        "\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "metadata": {
        "id": "4an6j6U6Vh7F",
        "outputId": "237b8d25-5b35-4bf0-e041-9e6a223dd1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-acd057a0432c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://github.com/ojask12/ai2-hw1/raw/main/data.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zAWPZOStVehJ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(112358)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q33RDBhVehK"
      },
      "source": [
        "### INSTRUCTIONS\n",
        "\n",
        "\n",
        "- This homework is a jupyter notebook. Download and work on it on your local machine.\n",
        "\n",
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropraite plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYYItLTWVehL"
      },
      "source": [
        "### Names of the people who worked on this homework together\n",
        "#### /name here/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-j-GHZxVehM"
      },
      "source": [
        "<a id=\"contents\"></a>\n",
        "\n",
        "## Notebook Contents\n",
        "\n",
        "- [**PART 1 [40 pts]: Model interpretation and predictive intervals in NN**](#part1)\n",
        "  - [Overview and Data Description](#part1intro)\n",
        "  - [Questions](#part1questions)\n",
        "  - [Solutions](#part1solutions)\n",
        "\n",
        "\n",
        "- [**PART 2.1 [30 pts]: Kannada MNIST Kaggle competition using ANNs**](#part2.1)\n",
        "  - [Problem Statement](#part2.1intro)\n",
        "  - [The Kannada MNIST Dataset](#part2.1about)\n",
        "  - [Downloading the Data Files](#part2.1data)\n",
        "  - [AI2-C2 Homework Kaggle Competition](#part2.1kaggle)\n",
        "  - [Questions](#part2.1questions)\n",
        "  - [Solutions](#part2.1solutions)\n",
        "\n",
        "- [**PART 2.2 [30 pts]: Kannada MNIST using CNNs**](#part2)\n",
        "  - [Questions](#part2.2questions)\n",
        "  - [Solutions](#part2.2solutions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCBGlB9GVehN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSijN5zPVeh3"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "<h1> PART 1 [40 pts]: Model interpretation and predictive intervals in NN </h1> \n",
        "\n",
        "<a id=\"part1intro\"></a>\n",
        "\n",
        "<b> Overview and Data Description </b>\n",
        "\n",
        "In this problem, you will be building and interpreting models to predict whether a flight was delayed for its arrival based on features that could be measured as the flight takes off.  \n",
        "We will also estimate the predictive intervals of the model using bootstrapping. We will utilize those predictive intervals to build a new kind of model: a model that refrains from making a prediction when it is not confident.  \n",
        "\n",
        "The included variables are:\n",
        "\n",
        "**ARRIVAL_DELAY**: the difference between scheduled arrival and actual arrival, in minutes (positive is late, negative is early).\n",
        "\n",
        "**DISTANCE**: the distance between arrival and departure airports, in miles.\n",
        "\n",
        "**SCHEDULED_TIME**: the flight's scheduled travel time.\n",
        "\n",
        "**MONTH**: the month the flight took off, 1 = January, 2 = February, etc.\n",
        "\n",
        "**SCHED_DEP_HOUR**: the scheduled departure time (the hour of the day).\n",
        "\n",
        "**SCHED_ARR_HOUR**: the scheduled arrival time (the hour of the day).\n",
        "\n",
        "**FLIGHT_COUNT**: the number of flights flying out of that airport before noon on a typical day.\n",
        "\n",
        "**DAY_OF_WEEK**: the day of the week, 1 = Monday, 2 = Tuesday, etc.\n",
        "\n",
        "**ORIGIN_AIRPORT**: the airport the flight took off from.\n",
        "\n",
        "**DESTINATION_AIRPORT**: the airport the flight was scheduled to land at.\n",
        "\n",
        "For the airport codes, see: https://www.bts.gov/topics/airlines-and-airports/world-airport-codes\n",
        "\n",
        "To sucessfully complete this part, you will proceed by fitting a NN model, evaluating its accuracy, interpreting the predictors' importance, and finally evaluating the predictive intervals.\n",
        "\n",
        "**NOTE:** The observations were sampled so that roughly half of the observations were delayed and half of the observations were not delayed.\n",
        "\n",
        "</div> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGZSmTc3Veh5"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "<h2>PART 1: Questions</h2> \n",
        "\n",
        "**1.1.1 [2 points]**  Read in the dataset `flights.csv`. Create a variable `DELAY_OR_NOT` that denotes whether `ARRIVAL_DELAY` is greater than or equal to 15 minutes (the FAA and BTS define a flight as delayed only if it arrives 15 minutes late or more). This is going to be the response variable for the rest of this question. \n",
        "\n",
        "**1.1.2 [2 points]** Preprocess the data: one-hot-encode the non-numeric categorical variables, deal with missing values if there are any, scale your data, and split the data into training and test sets (use an 80/20 split with `random_state=111`). Print the resulting shapes of your $X$ and $y$ dataframes for both your train and your test sets.\n",
        "\n",
        "**1.2 [2 points]** Fit an artificial neural network model using all predictors (name this model `NN_model`).  Use a dense feed-forward network with two hidden layers with 15 nodes in each hidden layer. For this network, use appropriate activation functions for each layer, select an appropriate loss function and optimizer, specify a validation split of 0.2, train for an appropriate number of epochs based on the results of your training and validation accuracy plot, and feel free to use the default batch size while training. Plot the training accuracy and validation accuracy as a function of epochs from your `NN_model` training history. Evaluate the `NN_model` model on both train and test, and print out the resulting train and test accuracies.\n",
        "\n",
        "**1.3 [10 points]** To begin our interpretation of the resulting `NN_model`, we will first use a \"proxy model\" that we know how to interpret and train it on our `NN_model` training predictions.\n",
        "\n",
        "- **1.3.1** For this we need to modify our training set. First, generate a set of `NN_model` class predictions for the training set. These training predictions will be used to form a revised training dataset for our proxy model: (a) use all of the same $X$ values used by `NN_model` for our $X$ train and (b) replace the actual response values $y$ with the predicted $\\hat{y}$ values generated by the fitted `NN_model`.\n",
        "\n",
        "- **1.3.2** Next, fit a logistic regression model using your revised training dataset from 1.3.1 (name this model `logreg`). Use ridge-like regularization. Print the `logreg` test accuracy to confirm that it is similar to what we saw for our `NN_model` test accuracy in 1.2. You may need to adjust `C` in order to achieve a similar accuracy.\n",
        "\n",
        "- **1.3.3** Now use sklearn's `permutation_importance` class (already included in this notebook's imports) to compute the feature importance using the `logreg` model.\n",
        "\n",
        "  - Read the official documentation for `permutation_importance` [here](https://scikit-learn.org/stable/modules/permutation_importance.html#:~:text=The%20permutation%20feature%20importance%20is,model%20depends%20on%20the%20feature.) as well as [here](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) to learn how it works.\n",
        "\n",
        "  - You can use the default number of `n_repeats` and your estimator's default `scorer`. To speed up the time it takes to run your permutations, you can try setting `n_jobs=-1` to take full advantage of all of your available processor cores.\n",
        "\n",
        "  - Measure the **relative** variable importance (i.e. as a proportion of the variable importance of the most important variable identified by `permutation_importance`) and generate a barplot illustrating the relative variable importances for the top-10 most important predictors identified using `permutation_importance`.\n",
        "\n",
        "**1.4 [10 points]** Another way to interpret the  `NN_model` is by examining the response as a function of any of the predictors. Particularly, we will select from features often found most significant from the analysis above. **For all 1.4 plots below**, for ease of interpretation, **please be certain to** display all predictors on their original scales. \n",
        "\n",
        "   - **1.4.1** Set all predictors to their means/modes except for `SCHED_DEP_HOUR`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` on the data from the **training set**. Interpret what you see in 2-4 sentences.\n",
        "   - **1.4.2** Set all predictors to their means/modes except for `SCHED_DEP_HOUR` and `FLIGHT_COUNT`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` and `FLIGHT_COUNT` from the training set (see the question 1.4 \"HINT\" below).\n",
        "\n",
        "   - **1.4.3**   Set all predictors to their means/modes except for except for `SCHED_DEP_HOUR` and `SCHED_ARR_HOUR`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` and `SCHED_ARR_HOUR` from the training set.\n",
        "\n",
        "\n",
        "   - **1.4.4** Set all predictors to their means/modes except for except for `SCHED_DEP_HOUR` and `DISTANCE`. Predict the probability of delay and plot the predicted probabilities of delay vs. `SCHED_DEP_HOUR` and `DISTANCE` from the training set. \n",
        "  - **1.4.5** In 5-10 sentences, interpret what you have seen in 1.4.2, 1.4.3, and 1.4.4.\n",
        "  \n",
        "**HINT:** For 1.4.2, 1.4.3, and 1.4.4, when you include `SCHED_DEP_HOUR` on one axis and your second predictor on the other axis, you can color your datapoints based on their corresponding predicted probabilities by using  the `c` and `cmap` arguments in `plt.scatter`. You can also add a labeled colorbar to your plot to make clear what those colors mean. Please refer to the matplotlib documentation for examples.\n",
        "    \n",
        "**1.5 [2 points]**\n",
        "    \n",
        "In this part, we will attempt to do model inference. Neural Networks have too many parameters, and therefore inference on all the parameters is intractable and meaningless. \n",
        "\n",
        "Using the same network architecture as `NN_model` (layers, nodes, activations, etc.) and your scaled data from that model, create multiple training sets using bootstrapping and fit a separate neural network model to each bootstrapped set of data (a minimum of at least 50 bootstraps should be used). Predict the output on the test data for each model. Randomly select 8 test observations and on 8 subplots, plot the distribution of predicted probabilities (i.e. $n$ bootstrapped probabilites) with the 95% CI bounds clearly marked and reported in each subplot and the actual class of each observation included in each subplot's title for easy reference.\n",
        "\n",
        "Interpret what you see in 3-5 sentences.\n",
        "\n",
        "    \n",
        "**NOTE:** The code for this problem can take an extremely long time to execute. Please feel free to use the provided `progressbar` function below to visually track the progress of your bootstraps.\n",
        "    \n",
        "**1.6 [12 points]**\n",
        "    \n",
        "Using the probability distribution of the predictions obtained from the bootstrapped samples above, we can evaluate how \"significant\" our bagged (i.e. bootstrap-aggregated) prediction will be for each test observation.\n",
        "\n",
        "To accomplish this, you will first calculate the ratio of bootstrapped probabilities that cross the threshold value of $\\hat{p}=0.5$. Let's call this ratio the **Posterior Prediction Ratio (PPR)**. When a bagged prediction's $PPR=0$, all predictions are compatible (i.e. all bootstrapped probabilities for that test observation are on the same side of $\\hat{p}=0.5$). Likewise, when the $PPR=0.5$, half of the bootstrapped predictions for that test observation are $\\hat{y}=0$, and the other half are $\\hat{y}=1$. After calculating your $PPR$ values for all test observations, you should have $n=2000$ $PPR$ values (i.e. one for each test observation).\n",
        "\n",
        "Next, to get more accurate predictions, we can create an **abstain** model that will abstain from making a prediction for a particular observation if some defined threshold for significance (i.e. maximum $PPR$ value) is not met. (If you'd like to learn more about abstain models, you can read more [here](https://openreview.net/forum?id=rJxF73R9tX).)\n",
        "\n",
        "Let's explore how your resulting test accuracies might change by using your bootstrapped prediction results from question 1.5 for an **abstain bagging model** (i.e. a bootstrap aggregated model where some test observations are simply not predicted based on a given $PPR$ threshold). You can make your abstain model *stricter* by using smaller $PPR$ threshold values.\n",
        "\n",
        "- Print the test accuracy for your **bagging model** predictions from question 1.5 using predictions for all 2,000 of our test observations. \n",
        "\n",
        "- Plot the test accuracies for an **abstain bagging model** using your predictions from question 1.5 as a function of increasing $PPR$.\n",
        "\n",
        "- Also, plot the proportion of test observations not abstained (i.e. the proportion of those predicted) for your **abstain bagging model** as a function of increasing $PPR$.\n",
        "\n",
        "- Interpret what you see in 3-5 sentences.\n",
        "\n",
        "    \n",
        "**NOTE**: You should observe that as $PPR$ decreases (more confident predictions), you must also compromise on the number of points that your abstain model predicts confidently. \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jaiW193VeiA"
      },
      "source": [
        "<a id=\"part1solutions\"></a>\n",
        "\n",
        "## PART 1: Solutions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFzUGMKDVeiC"
      },
      "source": [
        "<div class='exercise-r'>  \n",
        "\n",
        "**1.1**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7X-QQ-M7VeiD"
      },
      "outputs": [],
      "source": [
        "# your code here \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sClikd_LVeiG"
      },
      "source": [
        "<div class='exercise-r'>  \n",
        "\n",
        "**1.1.2**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KNL05wnHVeiI"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XjJ1jkuVeiJ"
      },
      "source": [
        "<div class='exercise-r'>  \n",
        "    \n",
        "**1.2**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ho-Tf1WGVeiK"
      },
      "outputs": [],
      "source": [
        "# build your NN \n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pbTgQFjeVeiK"
      },
      "outputs": [],
      "source": [
        "# compile it and run it\n",
        "# your code here \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FUM_WkQVVeiO"
      },
      "outputs": [],
      "source": [
        "# plot train and val acc as  a function of epochs\n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CvgEa0rGVeiP"
      },
      "outputs": [],
      "source": [
        "# primer to print: \n",
        "# print(\"NN_model_train_auc:\", roc_auc_score(y_train, y_hat))\n",
        "# your code here \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9Bsw3xJuVeiQ"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_H4_gNKVeiS"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.3**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bPI82KfYVeiS"
      },
      "outputs": [],
      "source": [
        "# Fit the logistic regression model\n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUgMYSdgVeiT"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.4**    \n",
        "\n",
        "**1.4.1**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wgxesSj6VeiU"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmsCyNV0VeiW"
      },
      "source": [
        "**INTERPRETATION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCWyYWZXVeiX"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekkBwTgaVeiX"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.4.2**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xdg7RRjCVeiY"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rEPU-ANVeiY"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.4.3**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "styIr2n1VeiY"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDNmUG05VeiZ"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.4.4**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cOFLEMXvVeiZ"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anLK8pYcVeia"
      },
      "source": [
        "**INTERPRETATION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzlscvI8Veia"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-HpxGNWVeib"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.5**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qRS0ey1yVeib"
      },
      "outputs": [],
      "source": [
        "def progressbar(n_step, n_total):\n",
        "    \"\"\"Prints self-updating progress bar to stdout to track for-loop progress\n",
        "    \n",
        "    There are entire 3rd-party libraries dedicated to custom progress-bars.\n",
        "    A simple function like this is often more than enough to get the job done.\n",
        "    \n",
        "    :param n_total: total number of expected for-loop iterations\n",
        "    :type n_total: int\n",
        "    :param n_step: current iteration number, starting at 0\n",
        "    :type n_step: int\n",
        "\n",
        "    .. example::\n",
        "    \n",
        "        for i in range(n_iterations):\n",
        "            progressbar(i, n_iterations)\n",
        "            \n",
        "    .. source:\n",
        "    \n",
        "        This function is a simplified version of code found here:\n",
        "        https://stackoverflow.com/questions/3160699/python-progress-bar/15860757#15860757\n",
        "    \"\"\"\n",
        "    n_step = n_step + 1\n",
        "    barlen = 50\n",
        "    progress = n_step / n_total\n",
        "    block = int(round(barlen * progress))\n",
        "    status = \"\"\n",
        "    if n_step == n_total:\n",
        "        status = \"Done...\\r\\n\\n\"\n",
        "    text = \"\\r [{0}] {1}/{2} {3}\".format(\n",
        "        \"=\" * block + \"-\" * (barlen - block),\n",
        "        n_step,\n",
        "        n_total,\n",
        "        status,\n",
        "    )\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7XWrNcJVeic",
        "outputId": "20da0c7f-068e-4945-d1a1-e3822d412203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\nWall time: 5.01 µs\n"
        }
      ],
      "source": [
        "%%time\n",
        "# Bootstrap and train your networks and get predictions on fixed X test\n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cRhIAT3CVeif"
      },
      "outputs": [],
      "source": [
        "# generate your plot\n",
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6X1qPStVeif"
      },
      "source": [
        "**INTERPRETATION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGD1aJmVeif"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV4MnIANVeig"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**1.6**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8G44cmJQVeig"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qKcLXT3Veih"
      },
      "source": [
        "**INTERPRETATION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7pF6yBWVeih"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVa3DUwLVeik"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "<h1>PART 2.1 [30 pts]: Kannada MNIST Kaggle competition using ANNs </h1>\n",
        "\n",
        "\n",
        "<a id=\"part2.1intro\"></a>\n",
        "<h2> Problem Statement </h2>\n",
        "\n",
        "ANNs can be prone to overfitting, where they learn specific patterns present in the training data, but the patterns do not generalize to new data.\n",
        "\n",
        "There are several methods used to improve ANN generalization. \n",
        "\n",
        "One approach is to use an architecture just barely wide or deep enough to fit the data. The idea here is that smaller networks are less expressive and thus less able to overfit the data.\n",
        "\n",
        "However, it is difficult to know a priori the correct size of the ANN, and it is computationally costly to hunt for the correct size. Given this, other methodologies are used to prevent overfitting and improve ANNs' generalizability. These methodologies, like other techniques that combat overfitting, fall under the umbrella term of \"regularization\".\n",
        "\n",
        "In this problem, you are asked to regularize a network of a given architecture.\n",
        "\n",
        "<a id=\"part2.1about\"></a>\n",
        "\n",
        "<h3> The Kannada MNIST Dataset </h3>\n",
        "\n",
        "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F1e01bcc28b5ccb7ad38a4ffefb13cde0%2Fwondu.png?generation=1603204077179447&alt=media\" style=\"float:right\">\n",
        "\n",
        "For this problem, we will be working with a modified version of [Kannada MNIST dataset](https://arxiv.org/pdf/1908.01242.pdf) , which is a large database of handwritten digits in the indigenous language *Kannada*.\n",
        "\n",
        "This dataset consists of 60,000 28x28 grayscale images of the ten digits, along with a test set of 10,000 images. \n",
        "\n",
        "For this homework, we will simplify the problem by only use the digits labeled `0` and `1` owing to the similarity of the two symbols, and we want to use a total of 1200 samples for training (this includes the data you will use for validation).\n",
        "\n",
        "To understand the dataset better, we recommend this [article](https://towardsdatascience.com/a-new-handwritten-digits-dataset-in-ml-town-kannada-mnist-69df0f2d1456) by Vinay Prabhu, the curator of the dataset.\n",
        "\n",
        "<a id=\"part2data\"></a>\n",
        "\n",
        "<h3> Downloading the Data Files </h3>\n",
        "\n",
        "Please download all files from Kaggle [using this link](https://www.kaggle.com/c/hw1ai2c3/data).\n",
        "\n",
        "Here's a brief description of the available files:\n",
        "\n",
        "- `kmnist_train.csv` is our training dataset and the last column contains our response class. The 784 other columns correspond to the pixel values of the 28x28 dimension image.\n",
        "\n",
        "Class 0 means a sample is the handwritten digit `0` and class 1 means a sample is the handwritten digit `1` in the Kannada language.  \n",
        "\n",
        "- `kmnist_test.csv` has a structure similar to `kmnist_train.csv`, however the class label column is NOT included in with the test set. `kmnist_test.csv` has 2000 samples. \n",
        "\n",
        "Kaggle leaderboard scores are accuracy scores calculated by Kaggle when you upload your predictions on this test set.\n",
        "\n",
        "- `sample_submission.csv` is the format that kaggle will accept.\n",
        "\n",
        "<a id=\"part2.1kaggle\"></a>\n",
        "\n",
        "<h3> AI2-C2 Homework Kaggle Competition </h3>\n",
        "\n",
        "You need to create an account on Kaggle and join the competition via this [link](https://www.kaggle.com/t/411d852ead864884a0b45eff201fa1f0). **This is a limited participation competition. Please DO NOT share this link.**\n",
        "\n",
        "For more information on the rules governing this Kaggle competition, **please [see question 2.1.3 below](#part2_3).**\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfuQwP8UVeik"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "<h2>PART 2.1 Questions</h2>\n",
        "\n",
        "**2.1.1 [5 points]** **Get the data:**\n",
        "\n",
        "- Download data from the competition page.\n",
        "- We will utilize `kmnist_test.csv` in question 2.1.3.4 only. \n",
        "- Load the data and use the matplotlib function `imshow` to display a handwritten 0 and a handwritten 1.\n",
        "\n",
        "**2.1.2 [10 points]** **Overfit an ANN:** \n",
        "\n",
        "Build a fully-connected network (FCN) with the architecture given below using `tensorflow.keras` and assign it to a variable called `model_overfit`:\n",
        "\n",
        "- Number of hidden layers: 3\n",
        "- Nodes per hidden layer: 100, 100, 100\n",
        "- Activation function: ReLU \n",
        "- Loss function: binary_crossentropy\n",
        "- Output unit: Sigmoid \n",
        "- Optimizer: adam (use the defaults; no other tuning)\n",
        "- Epochs: no more than 2,000\n",
        "- Batch size: 128\n",
        "- Validation size: 0.3\n",
        "\n",
        "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F6a491ff8d4ff590dc8ded9a25461cd4b%2FScreenshot%202020-10-20%20at%209.42.36%20PM.png?generation=1603210420701577&alt=media) \n",
        "    \n",
        "This ANN, when trained on the dataset, will overfit to the training set. Plot the training accuracy and validation accuracy (the x-axis should represent the number of epochs, and the y-axis should represent the accuracy). Explain how you can tell the model is overfitting. \n",
        "\n",
        "<a id=\"part2_3\"></a>\n",
        "\n",
        "**2.1.3 [15 points]** **Regularize overfit network:**\n",
        "\n",
        "Create an ANN that doesn't overfit and compete on Kaggle.\n",
        "\n",
        "**DON'TS**\n",
        "\n",
        "**Don't change the architecture**. In other words, keep the number of layers, number of nodes, activation function,  loss function and output unit the same. **No CNNs/RNNs/enhancements allowed for the competition.**\n",
        "\n",
        "    \n",
        "**NOTE**: We strongly discourage you to use a different training set than the one provided to you (Data augmentation is allowed). If the test set accuracy of your model in this notebook is significantly different than your kaggle submission score, you will receive zero credit for this segment of the homework.\n",
        "\n",
        "    \n",
        "**DOS**\n",
        "\n",
        "You can change the number of epochs (max 2000), batch size, optimizer, and of course, add elements that can help to regularize (e.g., dropout, L2 norm, etc.). You can also do data augmentation. \n",
        "\n",
        "\n",
        "- **2.1.3.1** Display your model summary and your training and validation accuracy and loss.\n",
        "\n",
        "\n",
        "- **2.1.3.2** Print the difference between the training and validation accuracies and the difference between the training and validation losses for the final trained epoch used by your model.\n",
        "\n",
        "\n",
        "- **2.1.3.3** Plot the training accuracy and validation accuracy as a function of epochs.\n",
        "\n",
        "\n",
        "- **2.1.3.4** Generate your test set class predictions using your regularized model. Save those predictions to a `.csv` formatted file. Submit that `.csv` file to this Kaggle Competition for leaderboard scoring.\n",
        "\n",
        "\n",
        "- **2.1.3.5** **Specify your Kaggle name that you have used on the leaderboard**. *We can't give you credit without this.*\n",
        "\n",
        "\n",
        "    \n",
        "**IMPORTANT NOTES ABOUT SCORING**:\n",
        "\n",
        "- The **public leaderboard** on kaggle only displays your performance on 50% of the test set.\n",
        "\n",
        "\n",
        "- After the competition is complete, the **private leaderboard** will show your performance on the FULL test set.\n",
        "    \n",
        "Only the **top 5** competitors (as ranked on the hidden private leaderboard) will be eligible for full credit on question 2.1.3 (out of **15 points**). Remaining competitors will be scored out of **10 points** only for 2.1.3.\n",
        "\n",
        "\n",
        "**ADDITIONAL RULES:**\n",
        "\n",
        "- Multiple Kaggle submissions are permitted, **just note** that you will need to choose, on Kaggle, which submission shall be used for final scoring.\n",
        "\n",
        "\n",
        "- The version of your final notebook submitted on edStem **must contain the same model** used to generate to your chosen Kaggle submission.\n",
        "\n",
        "\n",
        "- **Please do not manually label your submissions.** In other words, the labels should only be the outcome of your model.\n",
        "\n",
        "\n",
        "- **No external data are allowed, please only use the KMNIST training data downloaded via the link above for training your model.**\n",
        "\n",
        "\n",
        "- **Do not** create multiple accounts on Kaggle.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gqu9QfOVeil"
      },
      "source": [
        "<a id=\"part2.1solutions\"></a>\n",
        "\n",
        "## PART 2.1 Solutions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3C7ccGgVeil"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.1** \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YuQbHUPUVeim"
      },
      "outputs": [],
      "source": [
        "# your code here "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzTBP447Veim"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.2**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9XZ01x7RVein"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aktCiaeZVein"
      },
      "source": [
        "**INTERPRETATION:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RjUWrFUVeip"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU0xK_3fVeiq"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.3** \n",
        "\n",
        "**2.1.3.1**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jQFkM5GrVeiq"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1-qms_5Veir"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.3.2**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bE3WC4HSVeir"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26TCcT9KVeis"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.3.3**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mfLTA5ScVeis"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7eSG1SPVeis"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.3.4**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DXSjdBIIVeiy"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYyH4sEfVeiz"
      },
      "source": [
        "<div class='exercise-r'>\n",
        "\n",
        "**2.1.3.5**\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fV6D6EmVeiz"
      },
      "source": [
        "**YOUR KAGGLE LEADERBOARD NAME:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_25VUHvVei0"
      },
      "source": [
        "*your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qWzQ10cVei1"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "<h1>PART 2.2 [30points]: KMNIST Classification using CNNs</h1>\n",
        "\n",
        "In this part of Homework, you will now contruct a CNN-based model in order to best classify the Kannada MNIST dataset.\n",
        "\n",
        "**2.2.1 [5 points]** Examine the dataset and prepare the data by appropriately standardizing, reshaping and type-checking. \n",
        "\n",
        "**2.2.2 [20 points]** Construct a simple CNN model - with not more than 10 layers. Please ensure that you use the following layers/parameters in order to contruct the model -\n",
        "1. Maxpooling\n",
        "2. Dense layers\n",
        "3. Regularization methods such as Adam, Drop out, Batch Normalization etc. \n",
        "\n",
        "**2.2.3 [5 points]** Perform error analysis on the predictions of your model and report classification accuracy. This should also include loss plots that allow for comparision of the model performance across the epochs. Conclusively, provide a detailed inference of why certain misclassifications would have taken place.\n",
        "\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dun5h_IBVei1"
      },
      "source": [
        "<a id=\"part2.2solutions\"></a>\n",
        "\n",
        "## PART 2.2 Solutions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Heh4fGmfVei1"
      },
      "source": [
        "**2.2.1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUcfZ8FMVei3"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ATGpygHVei3"
      },
      "source": [
        "**2.2.2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ydetCCaVei3"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1GorcSeVei4"
      },
      "source": [
        "**2.2.3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW0CZLd8Vei4"
      },
      "outputs": [],
      "source": [
        "#your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCzohQXVei4"
      },
      "source": [
        "**Error Analysis Inference:**\n",
        "\n",
        "*your answer here*"
      ]
    }
  ]
}